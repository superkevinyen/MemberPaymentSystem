# MPS éƒ¨ç½²èˆ‡é‹ç¶­æŒ‡å—

## ğŸ“‹ ç›®éŒ„
- [1. ç’°å¢ƒæº–å‚™](#1-ç’°å¢ƒæº–å‚™)
- [2. éƒ¨ç½²æµç¨‹](#2-éƒ¨ç½²æµç¨‹)
- [3. é…ç½®ç®¡ç†](#3-é…ç½®ç®¡ç†)
- [4. ç›£æ§é‹ç¶­](#4-ç›£æ§é‹ç¶­)
- [5. æ•…éšœæ’é™¤](#5-æ•…éšœæ’é™¤)
- [6. ç¶­è­·æ“ä½œ](#6-ç¶­è­·æ“ä½œ)
- [7. æ€§èƒ½èª¿å„ª](#7-æ€§èƒ½èª¿å„ª)
- [8. å®‰å…¨é‹ç¶­](#8-å®‰å…¨é‹ç¶­)

---

## 1. ç’°å¢ƒæº–å‚™

### 1.1 åŸºç¤è¨­æ–½éœ€æ±‚

#### ğŸ–¥ï¸ ç¡¬ä»¶é…ç½®å»ºè­°

| ç’°å¢ƒ | CPU | å…§å­˜ | å­˜å„² | ç¶²çµ¡ | ç¯€é»æ•¸é‡ |
|------|-----|------|------|------|----------|
| **é–‹ç™¼ç’°å¢ƒ** | 2 vCPU | 4GB | 50GB SSD | 100Mbps | 1 |
| **æ¸¬è©¦ç’°å¢ƒ** | 4 vCPU | 8GB | 100GB SSD | 1Gbps | 2 |
| **ç”Ÿç”¢ç’°å¢ƒ** | 8 vCPU | 16GB | 500GB SSD | 10Gbps | 3+ |
| **ç½å‚™ç’°å¢ƒ** | 8 vCPU | 16GB | 500GB SSD | 10Gbps | 3+ |

#### ğŸŒ ç¶²çµ¡æ¶æ§‹
```mermaid
graph TB
    subgraph "å¤–éƒ¨ç¶²çµ¡"
        INTERNET[Internet]
        CDN[CDN]
    end
    
    subgraph "DMZ å€åŸŸ"
        LB[è² è¼‰å‡è¡¡å™¨]
        WAF[Web æ‡‰ç”¨é˜²ç«ç‰†]
    end
    
    subgraph "æ‡‰ç”¨å€åŸŸ"
        APP1[æ‡‰ç”¨æœå‹™å™¨1]
        APP2[æ‡‰ç”¨æœå‹™å™¨2]
        APP3[æ‡‰ç”¨æœå‹™å™¨3]
    end
    
    subgraph "æ•¸æ“šå€åŸŸ"
        DB_MASTER[ä¸»æ•¸æ“šåº«]
        DB_SLAVE1[å¾æ•¸æ“šåº«1]
        DB_SLAVE2[å¾æ•¸æ“šåº«2]
        REDIS[Redis é›†ç¾¤]
    end
    
    subgraph "ç®¡ç†å€åŸŸ"
        MONITOR[ç›£æ§æœå‹™]
        LOG[æ—¥èªŒæœå‹™]
        BACKUP[å‚™ä»½æœå‹™]
    end
    
    INTERNET --> CDN
    CDN --> WAF
    WAF --> LB
    LB --> APP1
    LB --> APP2
    LB --> APP3
    
    APP1 --> DB_MASTER
    APP2 --> DB_SLAVE1
    APP3 --> DB_SLAVE2
    APP1 --> REDIS
    APP2 --> REDIS
    APP3 --> REDIS
    
    DB_MASTER --> DB_SLAVE1
    DB_MASTER --> DB_SLAVE2
    
    APP1 --> MONITOR
    APP2 --> MONITOR
    APP3 --> MONITOR
    
    MONITOR --> LOG
    MONITOR --> BACKUP
```

### 1.2 è»Ÿä»¶ç’°å¢ƒ

#### ğŸ“¦ ä¾è³´è»Ÿä»¶æ¸…å–®
```yaml
# åŸºç¤è»Ÿä»¶
operating_system: "Ubuntu 22.04 LTS"
container_runtime: "Docker 24.0+"
orchestration: "Kubernetes 1.28+"
reverse_proxy: "Nginx 1.24+"

# æ•¸æ“šåº«
database: "PostgreSQL 15+"
cache: "Redis 7.0+"
search: "Elasticsearch 8.0+"

# ç›£æ§
monitoring: "Prometheus + Grafana"
logging: "ELK Stack"
tracing: "Jaeger"

# å®‰å…¨
ssl_certificate: "Let's Encrypt / å•†æ¥­è­‰æ›¸"
secrets_management: "HashiCorp Vault"
backup: "Restic + S3"
```

#### ğŸ”§ ç’°å¢ƒè®Šé‡é…ç½®
```bash
# .env.production
# Supabase é…ç½®
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key

# æ•¸æ“šåº«é…ç½®
DATABASE_URL=postgresql://user:password@host:5432/mps_production
DATABASE_POOL_SIZE=20
DATABASE_TIMEOUT=30

# Redis é…ç½®
REDIS_URL=redis://redis-cluster:6379
REDIS_POOL_SIZE=10

# å®‰å…¨é…ç½®
JWT_SECRET=your-jwt-secret
ENCRYPTION_KEY=your-encryption-key
API_RATE_LIMIT=1000

# ç›£æ§é…ç½®
PROMETHEUS_ENDPOINT=http://prometheus:9090
GRAFANA_URL=http://grafana:3000
LOG_LEVEL=info

# å¤–éƒ¨æœå‹™
WECHAT_APP_ID=your-wechat-app-id
WECHAT_APP_SECRET=your-wechat-app-secret
ALIPAY_APP_ID=your-alipay-app-id
```

---

## 2. éƒ¨ç½²æµç¨‹

### 2.1 Supabase éƒ¨ç½²

#### ğŸš€ Supabase é …ç›®åˆå§‹åŒ–
```bash
# 1. å‰µå»º Supabase é …ç›®
npx supabase init

# 2. å•Ÿå‹•æœ¬åœ°é–‹ç™¼ç’°å¢ƒ
npx supabase start

# 3. æ‡‰ç”¨æ•¸æ“šåº«é·ç§»
npx supabase db reset

# 4. éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
npx supabase db push --linked
```

#### ğŸ“Š æ•¸æ“šåº«åˆå§‹åŒ–è…³æœ¬
```sql
-- init_production.sql
-- 1. å‰µå»ºæ“´å±•
CREATE EXTENSION IF NOT EXISTS "pgcrypto";
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- 2. åŸ·è¡Œ schema æ–‡ä»¶
\i schema/mps_schema.sql

-- 3. åŸ·è¡Œ RPC æ–‡ä»¶
\i rpc/mps_rpc.sql

-- 4. å‰µå»ºåˆå§‹æ•¸æ“š
INSERT INTO app.membership_levels(level, name, min_points, max_points, discount, is_active) VALUES
  (0, 'æ™®é€šæœƒå“¡', 0, 999, 1.000, true),
  (1, 'éŠ€å¡æœƒå“¡', 1000, 4999, 0.950, true),
  (2, 'é‡‘å¡æœƒå“¡', 5000, 9999, 0.900, true),
  (3, 'é‘½çŸ³æœƒå“¡', 10000, null, 0.850, true)
ON CONFLICT (level) DO NOTHING;

-- 5. å‰µå»ºç³»çµ±ç”¨æˆ¶
INSERT INTO auth.users (id, email, encrypted_password, email_confirmed_at, created_at, updated_at)
VALUES (
  gen_random_uuid(),
  'admin@mps.system',
  crypt('admin_password', gen_salt('bf')),
  now(),
  now(),
  now()
) ON CONFLICT (email) DO NOTHING;
```

### 2.2 å®¹å™¨åŒ–éƒ¨ç½²

#### ğŸ³ Docker é…ç½®
```dockerfile
# Dockerfile.production
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

FROM node:18-alpine AS runtime

# å®‰å…¨é…ç½®
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nextjs -u 1001

WORKDIR /app

# è¤‡è£½æ§‹å»ºç”¢ç‰©
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json

# è¨­ç½®æ¬Šé™
USER nextjs

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:3000/health || exit 1

EXPOSE 3000

CMD ["npm", "start"]
```

#### ğŸš¢ Docker Compose é…ç½®
```yaml
# docker-compose.production.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.production
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
    depends_on:
      - redis
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - mps-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    restart: unless-stopped
    networks:
      - mps-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
      - nginx_logs:/var/log/nginx
    depends_on:
      - app
    restart: unless-stopped
    networks:
      - mps-network

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    networks:
      - mps-network

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - mps-network

volumes:
  redis_data:
  nginx_logs:
  prometheus_data:
  grafana_data:

networks:
  mps-network:
    driver: bridge
```

### 2.3 Kubernetes éƒ¨ç½²

#### âš™ï¸ K8s éƒ¨ç½²é…ç½®
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: mps-production

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mps-config
  namespace: mps-production
data:
  NODE_ENV: "production"
  LOG_LEVEL: "info"
  API_RATE_LIMIT: "1000"

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mps-secrets
  namespace: mps-production
type: Opaque
data:
  supabase-url: <base64-encoded-url>
  supabase-service-key: <base64-encoded-key>
  jwt-secret: <base64-encoded-secret>
  redis-password: <base64-encoded-password>

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mps-app
  namespace: mps-production
  labels:
    app: mps
    version: v1.0.0
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mps
  template:
    metadata:
      labels:
        app: mps
        version: v1.0.0
    spec:
      containers:
      - name: mps-app
        image: mps:latest
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          valueFrom:
            configMapKeyRef:
              name: mps-config
              key: NODE_ENV
        - name: SUPABASE_URL
          valueFrom:
            secretKeyRef:
              name: mps-secrets
              key: supabase-url
        - name: SUPABASE_SERVICE_ROLE_KEY
          valueFrom:
            secretKeyRef:
              name: mps-secrets
              key: supabase-service-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: app-logs
          mountPath: /app/logs
      volumes:
      - name: app-logs
        emptyDir: {}

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mps-service
  namespace: mps-production
spec:
  selector:
    app: mps
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
  type: ClusterIP

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mps-ingress
  namespace: mps-production
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - api.mps.example.com
    secretName: mps-tls
  rules:
  - host: api.mps.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mps-service
            port:
              number: 80

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mps-hpa
  namespace: mps-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mps-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### 2.4 CI/CD æµæ°´ç·š

#### ğŸ”„ GitHub Actions å·¥ä½œæµ
```yaml
# .github/workflows/deploy.yml
name: Deploy MPS to Production

on:
  push:
    branches: [main]
    tags: ['v*']
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: mps_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run linting
      run: npm run lint
    
    - name: Run type checking
      run: npm run type-check
    
    - name: Run unit tests
      run: npm run test:unit
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/mps_test
        REDIS_URL: redis://localhost:6379
    
    - name: Run integration tests
      run: npm run test:integration
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/mps_test
        REDIS_URL: redis://localhost:6379
    
    - name: Run security audit
      run: npm audit --audit-level high

  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.production
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to Staging
      run: |
        echo "Deploying to staging environment..."
        # éƒ¨ç½²åˆ°æ¸¬è©¦ç’°å¢ƒçš„è…³æœ¬
        ./scripts/deploy-staging.sh
      env:
        KUBECONFIG: ${{ secrets.STAGING_KUBECONFIG }}
        IMAGE_TAG: ${{ github.sha }}

  deploy-production:
    needs: [build, deploy-staging]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    environment: production
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to Production
      run: |
        echo "Deploying to production environment..."
        # éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒçš„è…³æœ¬
        ./scripts/deploy-production.sh
      env:
        KUBECONFIG: ${{ secrets.PRODUCTION_KUBECONFIG }}
        IMAGE_TAG: ${{ github.ref_name }}
    
    - name: Run smoke tests
      run: |
        echo "Running smoke tests..."
        ./scripts/smoke-tests.sh
      env:
        API_ENDPOINT: https://api.mps.example.com
        API_KEY: ${{ secrets.PRODUCTION_API_KEY }}
    
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

---

## 3. é…ç½®ç®¡ç†

### 3.1 ç’°å¢ƒé…ç½®

#### ğŸ”§ é…ç½®æ–‡ä»¶çµæ§‹
```
config/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ development.yml
â”‚   â”œâ”€â”€ staging.yml
â”‚   â””â”€â”€ production.yml
â”œâ”€â”€ secrets/
â”‚   â”œâ”€â”€ development.env
â”‚   â”œâ”€â”€ staging.env
â”‚   â””â”€â”€ production.env
â””â”€â”€ nginx/
    â”œâ”€â”€ nginx.conf
    â”œâ”€â”€ ssl/
    â””â”€â”€ sites-available/
```

#### ğŸ“ ç”Ÿç”¢ç’°å¢ƒé…ç½®
```yaml
# config/environments/production.yml
app:
  name: "MPS Production"
  version: "1.0.0"
  port: 3000
  log_level: "info"
  
database:
  pool_size: 20
  timeout: 30000
  ssl: true
  
redis:
  pool_size: 10
  timeout: 5000
  
security:
  rate_limit:
    window_ms: 60000
    max_requests: 1000
  cors:
    origin: ["https://app.mps.example.com"]
    credentials: true
  
monitoring:
  prometheus:
    enabled: true
    port: 9090
  health_check:
    enabled: true
    interval: 30
  
features:
  qr_rotation:
    enabled: true
    default_ttl: 900
  payment_retry:
    enabled: true
    max_attempts: 3
  audit_logging:
    enabled: true
    retention_days: 2555  # 7å¹´
```

#### ğŸ” å¯†é‘°ç®¡ç†
```bash
#!/bin/bash
# scripts/manage-secrets.sh

# ä½¿ç”¨ HashiCorp Vault ç®¡ç†å¯†é‘°
vault_write() {
    local path=$1
    local key=$2
    local value=$3
    
    vault kv put secret/mps/$path $key="$value"
}

vault_read() {
    local path=$1
    local key=$2
    
    vault kv get -field=$key secret/mps/$path
}

# åˆå§‹åŒ–ç”Ÿç”¢ç’°å¢ƒå¯†é‘°
init_production_secrets() {
    vault_write "production/database" "url" "$DATABASE_URL"
    vault_write "production/supabase" "service_key" "$SUPABASE_SERVICE_ROLE_KEY"
    vault_write "production/jwt" "secret" "$JWT_SECRET"
    vault_write "production/encryption" "key" "$ENCRYPTION_KEY"
    vault_write "production/redis" "password" "$REDIS_PASSWORD"
}

# è¼ªæ›å¯†é‘°
rotate_secrets() {
    local environment=$1
    
    # ç”Ÿæˆæ–°çš„ JWT å¯†é‘°
    new_jwt_secret=$(openssl rand -base64 32)
    vault_write "$environment/jwt" "secret" "$new_jwt_secret"
    
    # ç”Ÿæˆæ–°çš„åŠ å¯†å¯†é‘°
    new_encryption_key=$(openssl rand -base64 32)
    vault_write "$environment/encryption" "key" "$new_encryption_key"
    
    echo "Secrets rotated for environment: $environment"
}
```

### 3.2 Nginx é…ç½®

#### ğŸŒ åå‘ä»£ç†é…ç½®
```nginx
# nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # æ—¥èªŒæ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';
    
    access_log /var/log/nginx/access.log main;
    
    # åŸºæœ¬è¨­ç½®
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 10M;
    
    # Gzip å£“ç¸®
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript 
               application/javascript application/xml+rss 
               application/json application/xml;
    
    # å®‰å…¨é ­
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    
    # ä¸Šæ¸¸æœå‹™å™¨
    upstream mps_backend {
        least_conn;
        server app1:3000 max_fails=3 fail_timeout=30s;
        server app2:3000 max_fails=3 fail_timeout=30s;
        server app3:3000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=payment:10m rate=5r/s;
    
    # ä¸»æœå‹™å™¨é…ç½®
    server {
        listen 80;
        server_name api.mps.example.com;
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name api.mps.example.com;
        
        # SSL é…ç½®
        ssl_certificate /etc/nginx/ssl/mps.crt;
        ssl_certificate_key /etc/nginx/ssl/mps.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 10m;
        
        # API è·¯ç”±
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://mps_backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
            
            # è¶…æ™‚è¨­ç½®
            proxy_connect_timeout 5s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # æ”¯ä»˜ API ç‰¹æ®Šé™æµ
        location /api/payment/ {
            limit_req zone=payment burst=10 nodelay;
            
            proxy_pass http://mps_backend;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # å¥åº·æª¢æŸ¥
        location /health {
            access_log off;
            proxy_pass http://mps_backend;
        }
        
        # éœæ…‹æ–‡ä»¶
        location /static/ {
            expires 1y;
            add_header Cache-Control "public, immutable";
            alias /var/www/static/;
        }
    }
}
```

---

## 4. ç›£æ§é‹ç¶­

### 4.1 ç›£æ§é«”ç³»

#### ğŸ“Š Prometheus é…ç½®
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # MPS æ‡‰ç”¨ç›£æ§
  - job_name: 'mps-app'
    static_configs:
      - targets: ['app1:3000', 'app2:3000', 'app3:3000']
    metrics_path: '/metrics'
    scrape_interval: 10s
    
  # PostgreSQL ç›£æ§
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    
  # Redis ç›£æ§
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    
  # Nginx ç›£æ§
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
    
  # ç¯€é»ç›£æ§
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

#### ğŸš¨ å‘Šè­¦è¦å‰‡
```yaml
# rules/mps-alerts.yml
groups:
- name: mps.rules
  rules:
  # æ‡‰ç”¨å¯ç”¨æ€§å‘Šè­¦
  - alert: MPSAppDown
    expr: up{job="mps-app"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "MPS application is down"
      description: "MPS application {{ $labels.instance }} has been down for more than 1 minute."
  
  # é«˜éŒ¯èª¤ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value }} errors per second."
  
  # éŸ¿æ‡‰æ™‚é–“å‘Šè­¦
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High response time"
      description: "95th percentile response time is {{ $value }} seconds."
  
  # æ•¸æ“šåº«é€£æ¥å‘Šè­¦
  - alert: DatabaseConnectionHigh
    expr: pg_stat_activity_count > 80
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High database connections"
      description: "Database has {{ $value }} active connections."
  
  # æ”¯ä»˜å¤±æ•—ç‡å‘Šè­¦
  - alert: PaymentFailureRate
    expr: rate(payment_transactions_total{status="failed"}[5m]) / rate(payment_transactions_total[5m]) > 0.05
    for: 3m
    labels:
      severity: critical
    annotations:
      summary: "High payment failure rate"
      description: "Payment failure rate is {{ $value | humanizePercentage }}."
  
  # QR ç¢¼éæœŸç‡å‘Šè­¦
  - alert: QRExpirationRate
    expr: rate(qr_codes_total{status="expired"}[5m]) / rate(qr_codes_total[5m]) > 0.3
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High QR code expiration rate"
      description: "QR code expiration rate is {{ $value | humanizePercentage }}."
```

#### ğŸ“ˆ Grafana å„€è¡¨æ¿
```json
{
  "dashboard": {
    "title": "MPS System Overview",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Payment Success Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(payment_transactions_total{status=\"completed\"}[5m]) / rate(payment_transactions_total[5m])",
            "legendFormat": "Success Rate"
          }
        ]
      },
      {
        "title": "Database Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_activity_count",
            "legendFormat": "Active Connections"
          },
          {
            "expr": "rate(pg_stat_database_tup_inserted[5m])",
            "legendFormat": "Inserts/sec"
          }
        ]
      }
    ]
  }
}
```

### 4.2 æ—¥èªŒç®¡ç†

#### ğŸ“ çµæ§‹åŒ–æ—¥èªŒé…ç½®
```javascript
// logger.js
const winston = require('winston');
const { ElasticsearchTransport } = require('winston-elasticsearch');

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  defaultMeta: {
    service: 'mps-api',
    version: process.env.APP_VERSION,
    environment: process.env.NODE_ENV
  },
  transports: [
    // æ§åˆ¶å°è¼¸å‡º
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    }),
    
    // æ–‡ä»¶è¼¸å‡º
    new winston.transports.File({
      filename: 'logs/error.log',
      level: 'error',
      maxsize: 10485760, // 10MB
      maxFiles: 5
    }),
    
    new winston.transports.File({
      filename: 'logs/combined.log',
      maxsize: 10485760, // 10MB
      maxFiles: 10
    }),
    
    // Elasticsearch è¼¸å‡º
    new ElasticsearchTransport({
      level: 'info',
      clientOpts: {
        node: process.env.ELASTICSEARCH_URL
      },
      index: 'mps-logs'
    })
  ]
});

// æ¥­å‹™æ—¥èªŒè¨˜éŒ„å™¨
const businessLogger = {
  payment: (action, data) => {
    logger.info('Payment operation', {
      category: 'payment',
      action,
      ...data
    });
  },
  
  qr: (action, data) => {
    logger.info('QR operation', {
      category: 'qr',
      action,
      ...data
    });
  },
  
  security: (action, data) => {
    logger.warn('Security event', {
      category: 'security',
      action,
      ...data
    });
  }
};

module.exports = { logger, businessLogger };
```

#### ğŸ” æ—¥èªŒåˆ†æè…³æœ¬
```python
# scripts/log-analysis.py
import json
import pandas as pd
from elasticsearch import Elasticsearch
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self, es_host):
        self.es = Elasticsearch([es_host])
    
    def analyze_payment_patterns(self, days=7):
        """åˆ†ææ”¯ä»˜æ¨¡å¼"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)
        
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"category": "payment"}},
                        {"range": {"@timestamp": {
                            "gte": start_time.isoformat(),
                            "lte": end_time.isoformat()
                        }}}
                    ]
                }
            },
            "aggs": {
                "hourly_volume": {
                    "date_histogram": {
                        "field": "@timestamp",
                        "interval": "hour"
                    }
                },
                "merchant_distribution": {
                    "terms": {"field": "merchant_id"}
                },
                "error_analysis": {
                    "terms": {"field": "error_code"}
                }
            }
        }
        
        result = self.es.search(index="mps-logs", body=query)
        return self.process_payment_analysis(result)
    
    def detect_anomalies(self):
        """æª¢æ¸¬ç•°å¸¸æ¨¡å¼"""
        # æª¢æ¸¬ç•°å¸¸é«˜é »çš„å¤±æ•—
        failed_payments_query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"category": "payment"}},
                        {"term": {"action": "failed"}},
                        {"range": {"@timestamp": {"gte": "now-1h"}}}
                    ]
                }
            },
            "aggs": {
                "error_codes": {
                    "terms": {"field": "error_code"}
                }
            }
        }
        
        # æª¢æ¸¬ç•°å¸¸å¤§é¡äº¤æ˜“
        large_payments_query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"category": "payment"}},
                        {"term": {"action": "completed"}},
                        {"range": {"amount": {"gte": 50000}}},
                        {"range": {"@timestamp": {"gte": "now-1h"}}}
                    ]
                }
            }
        }
        
        failed_result = self.es.search(index="mps-logs", body=failed_payments_query)
        large_result = self.es.search(index="mps-logs", body=large_payments_query)
        
        return {
            "failed_payments": failed_result,
            "large_payments": large_result
        }
    
    def generate_daily_report(self):
        """ç”Ÿæˆæ—¥å ±"""
        yesterday = datetime.now() - timedelta(days=1)
        
        report = {
            "date": yesterday.strftime("%Y-%m-%d"),
            "payment_stats": self.get_payment_stats(yesterday),
            "error_summary": self.get_error_summary(yesterday),
            "performance_metrics": self.get_performance_metrics(yesterday)
        }
        
        return report
```

---

## 5. æ•…éšœæ’é™¤

### 5.1 å¸¸è¦‹å•é¡Œè¨ºæ–·

#### ğŸ”§ è¨ºæ–·è…³æœ¬
```bash
#!/bin/bash
# scripts/diagnose.sh

# é¡è‰²å®šç¾©
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ—¥èªŒå‡½æ•¸
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æª¢æŸ¥ç³»çµ±ç‹€æ…‹
check_system_health() {
    log_info "Checking system health..."
    
    # æª¢æŸ¥æ‡‰ç”¨æœå‹™
    if curl -f http://localhost:3000/health > /dev/null 2>&1; then
        log_info "Application is healthy"
    else
        log_error "Application health check failed"
        return 1
    fi
    
    # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
    if pg_isready -h localhost -p 5432 > /dev/null 2>&1; then
        log_info "Database is accessible"
    else
        log_error "Database connection failed"
        return 1
    fi
    
    # æª¢æŸ¥ Redis é€£æ¥
    if redis-cli ping > /dev/null 2>&1; then
        log_info "Redis is accessible"
    else
        log_error "Redis connection failed"
        return 1
    fi
    
    return 0
}

# æª¢æŸ¥æ”¯ä»˜åŠŸèƒ½
check_payment_functionality() {
    log_info "Checking payment functionality..."
    
    # æ¸¬è©¦ QR ç¢¼ç”Ÿæˆ
    local qr_response=$(curl -s -X POST http://localhost:3000/api/qr/rotate \
        -H "Content-Type: application/json" \
        -d '{"card_id":"test-card-id","ttl_seconds":300}')
    
    if echo "$qr_response" | jq -e '.qr_plain' > /dev/null 2>&1; then
        log_info "QR code generation is working"
    else
        log_error "QR code generation failed"
        echo "Response: $qr_response"
        return 1
    fi
    
    return 0
}

# æª¢æŸ¥æ•¸æ“šåº«æ€§èƒ½
check_database_performance() {
    log_info "Checking database performance..."
    
    # æª¢æŸ¥æ´»èºé€£æ¥æ•¸
    local active_connections=$(psql -t -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';" 2>/dev/null | xargs)
    
    if [ "$active_connections" -gt 50 ]; then
        log_warn "High number of active connections: $active_connections"
    else
        log_info "Active connections: $active_connections"
    fi
    
    # æª¢æŸ¥æ…¢æŸ¥è©¢
    local slow_queries=$(psql -t -c "SELECT count(*) FROM pg_stat_statements WHERE mean_time > 1000;" 2>/dev/null | xargs)
    
    if [ "$slow_queries" -gt 0 ]; then
        log_warn "Found $slow_queries slow queries"
        psql -c "SELECT query, calls, mean_time FROM pg_stat_statements WHERE mean_time > 1000 ORDER BY mean_time DESC LIMIT 5;"
    else
        log_info "No slow queries detected"
    fi
}

# æª¢æŸ¥ç£ç›¤ç©ºé–“
check_disk_space() {
    log_info "Checking disk space..."
    
    local disk_usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
    
    if [ "$disk_usage" -gt 80 ]; then
        log_error "Disk usage is high: ${disk_usage}%"
        return 1
    elif [ "$disk_usage" -gt 70 ]; then
        log_warn "Disk usage is moderate: ${disk_usage}%"
    else
        log_info "Disk usage is normal: ${disk_usage}%"
    fi
    
    return 0
}

# æª¢æŸ¥å…§å­˜ä½¿ç”¨
check_memory_usage() {
    log_info "Checking memory usage..."
    
    local memory_usage=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
    
    if [ "$memory_usage" -gt 90 ]; then
        log_error "Memory usage is critical: ${memory_usage}%"
        return 1
    elif [ "$memory_usage" -gt 80 ]; then
        log_warn "Memory usage is high: ${memory_usage}%"
    else
        log_info "Memory usage is normal: ${memory_usage}%"
    fi
    
    return 0
}

# ä¸»è¨ºæ–·å‡½æ•¸
main() {
    log_info "Starting MPS system diagnosis..."
    
    local exit_code=0
    
    check_system_health || exit_code=1
    check_payment_functionality || exit_code=1
    check_database_performance || exit_code=1
    check_disk_space || exit_code=1
    check_memory_usage || exit_code=1
    
    if [ $exit_code -eq 0 ]; then
        log_info "All checks passed successfully"
    else
        log_error "Some checks failed. Please review the output above."
    fi
    
    exit $exit_code
}

# åŸ·è¡Œè¨ºæ–·
main "$@"
```

### 5.2 æ•…éšœæ¢å¾©æµç¨‹

#### ğŸ”„ è‡ªå‹•æ¢å¾©è…³æœ¬
```python
# scripts/auto-recovery.py
import subprocess
import time
import logging
import requests
from typing import Dict, List

class AutoRecovery:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.recovery_actions = {
            'app_down': self.restart_application,
            'db_connection_failed': self.restart_database_connection,
            'high_memory': self.restart_application,
            'disk_full': self.cleanup_logs,
            'payment_failures': self.restart_payment_service
        }
    
    def detect_issues(self) -> List[str]:
        """æª¢æ¸¬ç³»çµ±å•é¡Œ"""
        issues = []
        
        # æª¢æŸ¥æ‡‰ç”¨å¥åº·ç‹€æ…‹
        try:
            response = requests.get('http://localhost:3000/health', timeout=5)
            if response.status_code != 200:
                issues.append('app_down')
        except requests.RequestException:
            issues.append('app_down')
        
        # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
        try:
            result = subprocess.run(['pg_isready', '-h', 'localhost'], 
                                  capture_output=True, timeout=5)
            if result.returncode != 0:
                issues.append('db_connection_failed')
        except subprocess.TimeoutExpired:
            issues.append('db_connection_failed')
        
        # æª¢æŸ¥å…§å­˜ä½¿ç”¨
        try:
            result = subprocess.run(['free'], capture_output=True, text=True)
            lines = result.stdout.split('\n')
            memory_line = lines[1].split()
            used = int(memory_line[2])
            total = int(memory_line[1])
            if (used / total) > 0.9:
                issues.append('high_memory')
        except Exception as e:
            self.logger.error(f"Memory check failed: {e}")
        
        # æª¢æŸ¥ç£ç›¤ç©ºé–“
        try:
            result = subprocess.run(['df', '/'], capture_output=True, text=True)
            lines = result.stdout.split('\n')
            disk_line = lines[1].split()
            usage_percent = int(disk_line[4].rstrip('%'))
            if usage_percent > 95:
                issues.append('disk_full')
        except Exception as e:
            self.logger.error(f"Disk check failed: {e}")
        
        return issues
    
    def restart_application(self):
        """é‡å•Ÿæ‡‰ç”¨æœå‹™"""
        self.logger.info("Restarting application...")
        try:
            # ä½¿ç”¨ systemd é‡å•Ÿæœå‹™
            subprocess.run(['systemctl', 'restart', 'mps-app'], check=True)
            time.sleep(30)  # ç­‰å¾…æœå‹™å•Ÿå‹•
            
            # é©—è­‰æœå‹™æ˜¯å¦æ­£å¸¸
            response = requests.get('http://localhost:3000/health', timeout=10)
            if response.status_code == 200:
                self.logger.info("Application restarted successfully")
                return True
            else:
                self.logger.error("Application restart failed - health check failed")
                return False
        except Exception as e:
            self.logger.error(f"Application restart failed: {e}")
            return False
    
    def restart_database_connection(self):
        """é‡å•Ÿæ•¸æ“šåº«é€£æ¥"""
        self.logger.info("Restarting database connection...")
        try:
            # é‡å•Ÿé€£æ¥æ± 
            subprocess.run(['systemctl', 'restart', 'pgbouncer'], check=True)
            time.sleep(10)
            
            # é©—è­‰é€£æ¥
            result = subprocess.run(['pg_isready', '-h', 'localhost'], 
                                  capture_output=True, timeout=5)
            if result.returncode == 0:
                self.logger.info("Database connection restored")
                return True
            else:
                self.logger.error("Database connection restart failed")
                return False
        except Exception as e:
            self.logger.error(f"Database connection restart failed: {e}")
            return False
    
    def cleanup_logs(self):
        """æ¸…ç†æ—¥èªŒæ–‡ä»¶"""
        self.logger.info("Cleaning up log files...")
        try:
            # æ¸…ç†èˆŠæ—¥èªŒ
            subprocess.run(['find', '/var/log', '-name', '*.log', 
                          '-mtime', '+7', '-delete'], check=True)
            
            # æ¸…ç†æ‡‰ç”¨æ—¥èªŒ
            subprocess.run(['find', '/app/logs', '-name', '*.log', 
                          '-mtime', '+3', '-delete'], check=True)
            
            # å£“ç¸®å¤§æ—¥èªŒæ–‡ä»¶
            subprocess.run(['find', '/var/log', '-name', '*.log', 
                          '-size', '+100M', '-exec', 'gzip', '{}', ';'], 
                         check=True)
            
            self.logger.info("Log cleanup completed")
            return True
        except Exception as e:
            self.logger.error(f"Log cleanup failed: {e}")
            return False
    
    def restart_payment_service(self):
        """é‡å•Ÿæ”¯ä»˜æœå‹™"""
        self.logger.info("Restarting payment service...")
        try:
            # æ¸…ç† Redis ç·©å­˜
            subprocess.run(['redis-cli', 'FLUSHDB'], check=True)
            
            # é‡å•Ÿæ‡‰ç”¨
            return self.restart_application()
        except Exception as e:
            self.logger.error(f"Payment service restart failed: {e}")
            return False
    
    def run_recovery(self):
        """åŸ·è¡Œè‡ªå‹•æ¢å¾©"""
        issues = self.detect_issues()
        
        if not issues:
            self.logger.info("No issues detected")
            return True
        
        self.logger.warning(f"Detected issues: {issues}")
        
        recovery_success = True
        for issue in issues:
            if issue in self.recovery_actions:
                self.logger.info(f"Attempting to recover from: {issue}")
                success = self.recovery_actions[issue]()
                if not success:
                    recovery_success = False
                    self.logger.error(f"Failed to recover from: {issue}")
            else:
                self.logger.warning(f"No recovery action defined for: {issue}")
                recovery_success = False
        
        return recovery_success

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    recovery = AutoRecovery()
    
    while True:
        try:
            recovery.run_recovery()
            time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            break
        except Exception as e:
            logging.error(f"Recovery process error: {e}")
            time.sleep(60)
```

---

## 6. ç¶­è­·æ“ä½œ

### 6.1 å®šæœŸç¶­è­·ä»»å‹™

#### ğŸ”„ ç¶­è­·è…³æœ¬
```bash
#!/bin/bash
# scripts/maintenance.sh

# è¨­ç½®ç¶­è­·æ¨¡å¼
enable_maintenance_mode() {
    echo "Enabling maintenance mode..."
    
    # å‰µå»ºç¶­è­·é é¢
    cat > /var/www/maintenance.html << EOF
<!DOCTYPE html>
<html>
<head>
    <title>ç³»çµ±ç¶­è­·ä¸­</title>
    <meta charset="utf-8">
</head>
<body>
    <h1>ç³»çµ±ç¶­è­·ä¸­</h1>
    <p>ç³»çµ±æ­£åœ¨é€²è¡Œç¶­è­·ï¼Œé è¨ˆ 30 åˆ†é˜å¾Œæ¢å¾©æœå‹™ã€‚</p>
    <p>å¦‚æœ‰ç·Šæ€¥æƒ…æ³ï¼Œè«‹è¯ç¹«å®¢æœã€‚</p>
</body>
</html>
EOF
    
    # æ›´æ–° Nginx é…ç½®
    cp /etc/nginx/sites-available/maintenance /etc/nginx/sites-enabled/default
    nginx -s reload
    
    echo "Maintenance mode enabled"
}

# ç¦ç”¨ç¶­è­·æ¨¡å¼
disable_maintenance_mode() {
    echo "Disabling maintenance mode..."
    
    # æ¢å¾©æ­£å¸¸é…ç½®
    cp /etc/nginx/sites-available/production /etc/nginx/sites-enabled/default
    nginx -s reload
    
    echo "Maintenance mode disabled"
}

# æ•¸æ“šåº«ç¶­è­·
database_maintenance() {
    echo "Starting database maintenance..."
    
    # æ›´æ–°çµ±è¨ˆä¿¡æ¯
    psql -c "ANALYZE;"
    
    # æ¸…ç†éæœŸæ•¸æ“š
    psql -c "DELETE FROM app.card_qr_history WHERE issued_at < NOW() - INTERVAL '90 days';"
    
    # é‡å»ºç´¢å¼•
    psql -c "REINDEX DATABASE mps_production;"
    
    # æ¸…ç† WAL æ–‡ä»¶
    psql -c "SELECT pg_switch_wal();"
    
    echo "Database maintenance completed"
}

# æ‡‰ç”¨ç¶­è­·
application_maintenance() {
    echo "Starting application maintenance..."
    
    # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    find /tmp -name "mps-*" -mtime +1 -delete
    
    # æ¸…ç†æ‡‰ç”¨ç·©å­˜
    redis-cli FLUSHDB
    
    # é‡å•Ÿæ‡‰ç”¨æœå‹™
    systemctl restart mps-app
    
    # ç­‰å¾…æœå‹™å•Ÿå‹•
    sleep 30
    
    # é©—è­‰æœå‹™ç‹€æ…‹
    if curl -f http://localhost:3000/health; then
        echo "Application maintenance completed successfully"
    else
        echo "Application maintenance failed - service not healthy"
        exit 1
    fi
}

# æ—¥èªŒç¶­è­·
log_maintenance() {
    echo "Starting log maintenance..."
    
    # æ­¸æª”èˆŠæ—¥èªŒ
    find /var/log/mps -name "*.log" -mtime +7 -exec gzip {} \;
    
    # åˆªé™¤éæœŸæ—¥èªŒ
    find /var/log/mps -name "*.gz" -mtime +30 -delete
    
    # æ¸…ç† Docker æ—¥èªŒ
    docker system prune -f
    
    echo "Log maintenance completed"
}

# å‚™ä»½ç¶­è­·
backup_maintenance() {
    echo "Starting backup maintenance..."
    
    # å‰µå»ºæ•¸æ“šåº«å‚™ä»½
    pg_dump mps_production | gzip > /backup/mps_$(date +%Y%m%d_%H%M%S).sql.gz
    
    # ä¸Šå‚³åˆ°é›²å­˜å„²
    aws s3 cp /backup/mps_$(date +%Y%m%d_%H%M%S).sql.gz s3://mps-backups/
    
    # æ¸…ç†æœ¬åœ°èˆŠå‚™ä»½
    find /backup -name "mps_*.sql.gz" -mtime +7 -delete
    
    echo "Backup maintenance completed"
}

# å®‰å…¨ç¶­è­·
security_maintenance() {
    echo "Starting security maintenance..."
    
    # æ›´æ–°ç³»çµ±åŒ…
    apt update && apt upgrade -y
    
    # æª¢æŸ¥å®‰å…¨æ¼æ´
    npm audit --audit-level high
    
    # è¼ªæ›å¯†é‘°
    ./rotate-secrets.sh production
    
    # æª¢æŸ¥ç•°å¸¸ç™»å…¥
    grep "Failed password" /var/log/auth.log | tail -20
    
    echo "Security maintenance completed"
}

# ä¸»ç¶­è­·å‡½æ•¸
main() {
    local maintenance_type=${1:-"full"}
    
    case $maintenance_type in
        "database")
            database_maintenance
            ;;
        "application")
            application_maintenance
            ;;
        "logs")
            log_maintenance
            ;;
        "backup")
            backup_maintenance
            ;;
        "security")
            security_maintenance
            ;;
        "full")
            enable_maintenance_mode
            database_maintenance
            application_maintenance
            log_maintenance
            backup_maintenance
            security_maintenance
            disable_maintenance_mode
            ;;
        *)
            echo "Usage: $0 {database|application|logs|backup|security|full}"
            exit 1
            ;;
    esac
    
    echo "Maintenance completed: $maintenance_type"
}

# åŸ·è¡Œç¶­è­·
main "$@"
```

### 6.2 æ•¸æ“šåº«ç¶­è­·

#### ğŸ—„ï¸ æ•¸æ“šåº«å„ªåŒ–è…³æœ¬
```sql
-- database_optimization.sql

-- 1. æ›´æ–°è¡¨çµ±è¨ˆä¿¡æ¯
ANALYZE app.transactions;
ANALYZE app.member_cards;
ANALYZE app.merchants;
ANALYZE app.card_qr_state;
ANALYZE audit.event_log;

-- 2. é‡å»ºç¢ç‰‡åŒ–ç´¢å¼•
REINDEX INDEX CONCURRENTLY idx_transactions_composite;
REINDEX INDEX CONCURRENTLY idx_tx_card_time;
REINDEX INDEX CONCURRENTLY idx_tx_merchant_time;

-- 3. æ¸…ç†éæœŸæ•¸æ“š
DELETE FROM app.card_qr_history 
WHERE issued_at < NOW() - INTERVAL '90 days';

DELETE FROM audit.event_log 
WHERE happened_at < NOW() - INTERVAL '2 years'
  AND action NOT IN ('PAYMENT', 'REFUND', 'RECHARGE');

-- 4. å„ªåŒ–è¡¨ç©ºé–“
VACUUM (ANALYZE, VERBOSE) app.transactions;
VACUUM (ANALYZE, VERBOSE) app.member_cards;
VACUUM (ANALYZE, VERBOSE) audit.event_log;

-- 5. æª¢æŸ¥è¡¨è†¨è„¹
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size
FROM pg_tables 
WHERE schemaname IN ('app', 'audit')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 6. æª¢æŸ¥æœªä½¿ç”¨çš„ç´¢å¼•
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes 
WHERE idx_scan = 0
ORDER BY schemaname, tablename;

-- 7. æª¢æŸ¥æ…¢æŸ¥è©¢
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
WHERE mean_time > 1000
ORDER BY mean_time DESC
LIMIT 10;
```

---

## 7. æ€§èƒ½èª¿å„ª

### 7.1 æ•¸æ“šåº«èª¿å„ª

#### âš¡ PostgreSQL é…ç½®å„ªåŒ–
```ini
# postgresql.conf ç”Ÿç”¢ç’°å¢ƒé…ç½®

# é€£æ¥è¨­ç½®
max_connections = 200
shared_buffers = 4GB
effective_cache_size = 12GB
work_mem = 64MB
maintenance_work_mem = 512MB

# WAL è¨­ç½®
wal_buffers = 64MB
checkpoint_completion_target = 0.9
wal_writer_delay = 200ms
commit_delay = 100

# æŸ¥è©¢å„ªåŒ–
random_page_cost = 1.1
effective_io_concurrency = 200
default_statistics_target = 1000

# æ—¥èªŒè¨­ç½®
log_min_duration_statement = 1000
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on

# è‡ªå‹•æ¸…ç†
autovacuum = on
autovacuum_max_workers = 4
autovacuum_naptime = 30s
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
```

### 7.2 æ‡‰ç”¨å±¤èª¿å„ª

#### ğŸš€ Node.js æ€§èƒ½å„ªåŒ–
```javascript
// performance-config.js
const cluster = require('cluster');
const os = require('os');

// é›†ç¾¤é…ç½®
if (cluster.isMaster) {
    const numCPUs = os.cpus().length;
    
    // å‰µå»ºå·¥ä½œé€²ç¨‹
    for (let i = 0; i < numCPUs; i++) {
        cluster.fork();
    }
    
    cluster.on('exit', (worker, code, signal) => {
        console.log(`Worker ${worker.process.pid} died`);
        cluster.fork();
    });
} else {
    // å·¥ä½œé€²ç¨‹é…ç½®
    const app = require('./app');
    
    // æ€§èƒ½å„ªåŒ–è¨­ç½®
    process.env.UV_THREADPOOL_SIZE = 128;
    
    // å…§å­˜ç›£æ§
    setInterval(() => {
        const memUsage = process.memoryUsage();
        if (memUsage.heapUsed > 1024 * 1024 * 1024) { // 1GB
            console.warn('High memory usage detected:', memUsage);
        }
    }, 60000);
    
    // å„ªé›…é—œé–‰
    process.on('SIGTERM', () => {
        console.log('SIGTERM received, shutting down gracefully');
        server.close(() => {
            process.exit(0);
        });
    });
    
    const server = app.listen(process.env.PORT || 3000);
}

// é€£æ¥æ± å„ªåŒ–
const poolConfig = {
    host: process.env.DB_HOST,
    port: process.env.DB_PORT,
    database: process.env.DB_NAME,
    user: process.env.DB_USER,
    password: process.env.DB_PASSWORD,
    
    // é€£æ¥æ± è¨­ç½®
    min: 5,
    max: 20,
    acquireTimeoutMillis: 30000,
    createTimeoutMillis: 30000,
    destroyTimeoutMillis: 5000,
    idleTimeoutMillis: 300000,
    reapIntervalMillis: 1000,
    createRetryIntervalMillis: 200,
    
    // æ€§èƒ½å„ªåŒ–
    propagateCreateError: false
};
```

---

## 8. å®‰å…¨é‹ç¶­

### 8.1 å®‰å…¨æª¢æŸ¥æ¸…å–®

#### ğŸ”’ æ—¥å¸¸å®‰å…¨æª¢æŸ¥
```bash
#!/bin/bash
# scripts/security-check.sh

# å®‰å…¨æª¢æŸ¥æ¸…å–®
security_checklist() {
    echo "=== MPS Security Checklist ==="
    
    # 1. æª¢æŸ¥ç³»çµ±æ›´æ–°
    echo "1. Checking system updates..."
    apt list --upgradable 2>/dev/null | grep -v "WARNING" | wc -l
    
    # 2. æª¢æŸ¥é–‹æ”¾ç«¯å£
    echo "2. Checking open ports..."
    netstat -tuln | grep LISTEN
    
    # 3. æª¢æŸ¥å¤±æ•—ç™»å…¥
    echo "3. Checking failed login attempts..."
    grep "Failed password" /var/log/auth.log | tail -10
    
    # 4. æª¢æŸ¥ SSL è­‰æ›¸
    echo "4. Checking SSL certificate..."
    openssl x509 -in /etc/nginx/ssl/mps.crt -text -noout | grep "Not After"
    
    # 5. æª¢æŸ¥æ–‡ä»¶æ¬Šé™
    echo "5. Checking file permissions..."
    find /app -type f -perm /o+w -ls
    
    # 6. æª¢æŸ¥é€²ç¨‹
    echo "6. Checking running processes..."
    ps aux | grep -E "(nginx|node|postgres|redis)" | grep -v grep
    
    # 7. æª¢æŸ¥ç£ç›¤åŠ å¯†
    echo "7. Checking disk encryption..."
    lsblk -f | grep crypto
    
    # 8. æª¢æŸ¥é˜²ç«ç‰†ç‹€æ…‹
    echo "8. Checking firewall status..."
    ufw status
    
    # 9. æª¢æŸ¥å…¥ä¾µæª¢æ¸¬
    echo "9. Checking intrusion detection..."
    if command -v fail2ban-client &> /dev/null; then
        fail2ban-client status
    fi
    
    # 10. æª¢æŸ¥å‚™ä»½ç‹€æ…‹
    echo "10. Checking backup status..."
    ls -la /backup/ | tail -5
}

# æ¼æ´æƒæ
vulnerability_scan() {
    echo "=== Vulnerability Scan ==="
    
    # NPM å®‰å…¨å¯©è¨ˆ
    echo "Running npm audit..."
    npm audit --audit-level high
    
    # Docker é¡åƒæƒæ
    echo "Scanning Docker images..."
    docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
        aquasec/trivy image mps:latest
    
    # ç³»çµ±æ¼æ´æƒæ
    echo "Scanning system vulnerabilities..."
    if command -v lynis &> /dev/null; then
        lynis audit system --quick
    fi
}

# å®‰å…¨é…ç½®æª¢æŸ¥
security_config_check() {
    echo "=== Security Configuration Check ==="
    
    # æª¢æŸ¥ SSH é…ç½®
    echo "Checking SSH configuration..."
    grep -E "PermitRootLogin|PasswordAuthentication|Port" /etc/ssh/sshd_config
    
    # æª¢æŸ¥ Nginx å®‰å…¨é ­
    echo "Checking Nginx security headers..."
    curl -I https://api.mps.example.com | grep -E "X-Frame-Options|X-Content-Type-Options|Strict-Transport-Security"
    
    # æª¢æŸ¥æ•¸æ“šåº«é…ç½®
    echo "Checking database security..."
    psql -c "SHOW ssl;" 2>/dev/null
    psql -c "SELECT name, setting FROM pg_settings WHERE name LIKE '%log%';" 2>/dev/null
}

# åŸ·è¡Œæ‰€æœ‰æª¢æŸ¥
main() {
    security_checklist
    echo ""
    vulnerability_scan
    echo ""
    security_config_check
}

main "$@"
```

### 8.2 äº‹ä»¶éŸ¿æ‡‰

#### ğŸš¨ å®‰å…¨äº‹ä»¶éŸ¿æ‡‰æµç¨‹
```python
# scripts/incident-response.py
import json
import smtplib
import requests
from datetime import datetime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class IncidentResponse:
    def __init__(self):
        self.severity_levels = {
            'LOW': 1,
            'MEDIUM': 2,
            'HIGH': 3,
            'CRITICAL': 4
        }
        
        self.response_teams = {
            'security': ['security@company.com'],
            'devops': ['devops@company.com'],
            'management': ['cto@company.com']
        }
    
    def detect_security_incident(self, log_entry: dict) -> dict:
        """æª¢æ¸¬å®‰å…¨äº‹ä»¶"""
        incident = None
        
        # æª¢æ¸¬æš´åŠ›ç ´è§£æ”»æ“Š
        if self.is_brute_force_attack(log_entry):
            incident = {
                'type': 'brute_force_attack',
                'severity': 'HIGH',
                'description': 'Multiple failed login attempts detected',
                'source_ip': log_entry.get('ip_address'),
                'timestamp': datetime.now().isoformat()
            }
        
        # æª¢æ¸¬ç•°å¸¸å¤§é¡äº¤æ˜“
        elif self.is_suspicious_transaction(log_entry):
            incident = {
                'type': 'suspicious_transaction',
                'severity': 'MEDIUM',
                'description': 'Unusually large transaction detected',
                'transaction_id': log_entry.get('transaction_id'),
                'amount': log_entry.get('amount'),
                'timestamp': datetime.now().isoformat()
            }
        
        # æª¢æ¸¬ SQL æ³¨å…¥å˜—è©¦
        elif self.is_sql_injection_attempt(log_entry):
            incident = {
                'type': 'sql_injection',
                'severity': 'CRITICAL',
                'description': 'SQL injection attempt detected',
                'source_ip': log_entry.get('ip_address'),
                'query': log_entry.get('query'),
                'timestamp': datetime.now().isoformat()
            }
        
        return incident
    
    def respond_to_incident(self, incident: dict):
        """éŸ¿æ‡‰å®‰å…¨äº‹ä»¶"""
        severity = incident['severity']
        incident_type = incident['type']
        
        # è¨˜éŒ„äº‹ä»¶
        self.log_incident(incident)
        
        # æ ¹æ“šåš´é‡ç¨‹åº¦æ¡å–è¡Œå‹•
        if severity == 'CRITICAL':
            self.critical_response(incident)
        elif severity == 'HIGH':
            self.high_response(incident)
        elif severity == 'MEDIUM':
            self.medium_response(incident)
        else:
            self.low_response(incident)
    
    def critical_response(self, incident: dict):
        """é—œéµäº‹ä»¶éŸ¿æ‡‰"""
        # ç«‹å³é€šçŸ¥æ‰€æœ‰åœ˜éšŠ
        self.notify_teams(['security', 'devops', 'management'], incident)
        
        # è‡ªå‹•é˜»æ–·
        if incident['type'] == 'sql_injection':
            self.block_ip(incident.get('source_ip'))
        
        # å•Ÿå‹•äº‹ä»¶éŸ¿æ‡‰æµç¨‹
        self.initiate_incident_response(incident)
    
    def high_response(self, incident: dict):
        """é«˜ç´šäº‹ä»¶éŸ¿æ‡‰"""
        # é€šçŸ¥å®‰å…¨å’Œé‹ç¶­åœ˜éšŠ
        self.notify_teams(['security', 'devops'], incident)
        
        # å¢å¼·ç›£æ§
        self.enhance_monitoring(incident)
        
        if incident['type'] == 'brute_force_attack':
            self.block_ip(incident.get('source_ip'))
    
    def medium_response(self, incident: dict):
        """ä¸­ç´šäº‹ä»¶éŸ¿æ‡‰"""
        # é€šçŸ¥å®‰å…¨åœ˜éšŠ
        self.notify_teams(['security'], incident)
        
        # è¨˜éŒ„è©³ç´°ä¿¡æ¯
        self.collect_evidence(incident)
    
    def low_response(self, incident: dict):
        """ä½ç´šäº‹ä»¶éŸ¿æ‡‰"""
        # åƒ…è¨˜éŒ„ï¼Œå®šæœŸå¯©æŸ¥
        self.log_for_review(incident)
    
    def block_ip(self, ip_address: str):
        """é˜»æ–· IP åœ°å€"""
        if ip_address:
            # æ·»åŠ åˆ°é˜²ç«ç‰†é»‘åå–®
            subprocess.run(['ufw', 'deny', 'from', ip_address])
            
            # æ·»åŠ åˆ° fail2ban
            subprocess.run(['fail2ban-client', 'set', 'sshd', 'banip', ip_address])
    
    def notify_teams(self, teams: list, incident: dict):
        """é€šçŸ¥éŸ¿æ‡‰åœ˜éšŠ"""
        for team in teams:
            if team in self.response_teams:
                self.send_alert_email(self.response_teams[team], incident)
                self.send_slack_notification(incident)
    
    def send_alert_email(self, recipients: list, incident: dict):
        """ç™¼é€å‘Šè­¦éƒµä»¶"""
        msg = MIMEMultipart()
        msg['From'] = 'security@company.com'
        msg['To'] = ', '.join(recipients)
        msg['Subject'] = f"Security Incident Alert - {incident['type']}"
        
        body = f"""
        Security Incident Detected
        
        Type: {incident['type']}
        Severity: {incident['severity']}
        Description: {incident['description']}
        Timestamp: {incident['timestamp']}
        
        Please investigate immediately.
        """
        
        msg.attach(MIMEText(body, 'plain'))
        
        # ç™¼é€éƒµä»¶
        server = smtplib.SMTP('localhost')
        server.send_message(msg)
        server.quit()
    
    def send_slack_notification(self, incident: dict):
        """ç™¼é€ Slack é€šçŸ¥"""
        webhook_url = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
        
        payload = {
            "text": f"ğŸš¨ Security Incident: {incident['type']}",
            "attachments": [
                {
                    "color": "danger" if incident['severity'] == 'CRITICAL' else "warning",
                    "fields": [
                        {"title": "Severity", "value": incident['severity'], "short": True},
                        {"title": "Type", "value": incident['type'], "short": True},
                        {"title": "Description", "value": incident['description'], "short": False}
                    ]
                }
            ]
        }
        
        requests.post(webhook_url, json=payload)
```

é€™å€‹éƒ¨ç½²èˆ‡é‹ç¶­æŒ‡å—ç‚º MPS ç³»çµ±æä¾›äº†å®Œæ•´çš„ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²ã€ç›£æ§ã€ç¶­è­·å’Œå®‰å…¨é‹ç¶­æ–¹æ¡ˆï¼Œç¢ºä¿ç³»çµ±èƒ½å¤ ç©©å®šã€å®‰å…¨åœ°é‹è¡Œåœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ã€‚

<update_todo_list>
<todos>
[x] æ·±å…¥åˆ†æç¾æœ‰ç³»çµ±æ¶æ§‹å’Œæ¥­å‹™é‚è¼¯
[x] å‰µå»ºå®Œæ•´çš„ç³»çµ±æ¶æ§‹åœ– (Mermaid)
[x] æ¢³ç†è©³ç´°çš„æ¥­å‹™éœ€æ±‚å’Œç”¨ä¾‹
[x] æ›´æ–° README.md åŒ…å«å®Œæ•´çš„ç³»çµ±æ¦‚è¿°
[x] æ›´æ–° docs/Architecture.md åŒ…å«è©³ç´°çš„æŠ€è¡“æ¶æ§‹
[x] å‰µå»ºæ•¸æ“šæµç¨‹åœ–å’Œæ¥­å‹™æµç¨‹åœ–
[x] è£œå……å®‰å…¨æ€§å’Œæ“´å±•æ€§åˆ†æ
[x] æ·»åŠ éƒ¨ç½²å’Œé‹ç¶­æŒ‡å—
</todos>
</update_todo_list>